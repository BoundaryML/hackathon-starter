/*************************************************************************************************

Welcome to Baml! To use this generated code, please run one of the following:

$ npm install @boundaryml/baml
$ yarn add @boundaryml/baml
$ pnpm add @boundaryml/baml

*************************************************************************************************/

// This file was generated by BAML: do not edit it. Instead, edit the BAML
// files and re-generate this code.
//
/* eslint-disable */
// tslint:disable
// @ts-nocheck
// biome-ignore format: autogenerated code
const fileMap = {
  
  "agent.baml": "// Results. (LLM Inputs)\n\nclass WeatherReport {\n  location string @description(\"Location of the weather report\")\n  temperature int @description(\"Temperature in Celsius\")\n  weather_status string @description(\"Description of the weather\")\n  timestamp int @description(\"Timestamp in Unix time\")\n}\n\nenum Role {\n  User\n  Assistant\n  Tool\n}\n\nclass Message {\n  role Role\n  content string\n  timestamp int @description(\"Timestamp in Unix time\")\n}\n\n// The state of the conversation that will be sent\n// to the agent. The client is responsible for updating\n// it with the result of tool calls.\nclass State {\n  weather_report WeatherReport?\n  recent_messages Message[]\n}\n\n// The query from the user.\nclass Query {\n  message string\n  timestamp int @description(\"Timestamp in Unix time\")\n}\n\nclass GetWeatherReport {\n  type \"get_weather_report\"\n  location string\n  @@stream.done\n}\n\nclass ComputeValue {\n  type \"compute_value\"\n  arithmetic_expression string\n  @@stream.done\n}\n\nclass MessageToUser {\n  type \"message_to_user\"\n  message string @stream.with_state\n}\n\n// Use this tool incatate that, in order to satisfy the\n// query, the client needs to update the state with the\n// result of the prior tool calls.\nclass Resume {\n  type \"resume\"\n  @@stream.done\n}\n\nfunction ChooseTools(state: State, query: Query) -> (GetWeatherReport | ComputeValue | MessageToUser | Resume)[] {\n\n  client CustomGPT4o\n  prompt #\"\n\n    {{ Instructions() }}\n\n    Current state: {{ state }}\n    Current query: {{ query }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\ntemplate_string Instructions() #\"\n  Lucky you, you're a friendly chatbot!\n\n  Have a conversation with the user. Our users are particularly interested in\n  the weather and in arithmetic.\n\n  You can enrich your understanding of the user and the world by issuing calls\n  arrording to the following schema.\n\n  If the user asks about the weather, you will need to issue the right tool call,\n  and in a subsequent call to you, you will have the weather report in your context.\n\n  IMPORTANT: If the weather report is already in the state, you don't need to issue\n  a get_weather_report call again (unless the report is very old).\n\n  In order to arrange for the new context to arrive, you may need to issue a \"resume\"\n  toll call. For example, if you are asked to add 5 to the current temperature, you\n  would need to issue both a get-weather call and a resume call. Then, you will\n  be called again with the updated state. In this case, your new tool calls should\n  be a compute-value call. Iff there is something to say about the computed value,\n  then you will need to use the Resume tool after the compute-value tool, but if\n  you just want to answer the question with the value, you don't need to Resume.\n\n  Before issuing the weather tool call, you may need to ask the user where they are\n  located.\n\n  IMPORTANT: Every tool call for which you need the client to update the state before you can\n\n  fullfil the full query should end in a resume call. That is, the output array\n  must have at least two elements - the pramiry tool call, and then a resume call.\n\n  NEVER just ask a question without a resume after, unless your question fully satisfies\n  what the user is requesting. If they ask for the weather, and you ask their location,\n  you MUST issue a second call in the array: resume.\n\n  IMPORTANT: If you issue the same tool call 2 times in a row, and are considering issuing\n  it a 3rd time, don't! Just apologize and say you don't know how to help.\n\n  IMPORTANT: If you want to call the weather tool, you must know the location that the\n  user wants to know the weather for. If you don't know, ask them, do not call the\n  weather tool. If you are asking a question, do not finish with the Resume tool.\n\n  If you notice that their current query appears to be a response to\n  your most recent question (in the recent_messages array), then go ahead and use\n  that information to call the weather tool.\n\n  If a query seems strange, be sure to examine the recent_messages array, to see if it\n  is explained by that context.\n  \"#\n\ntest WeatherToolInit() {\n  functions [ChooseTools]\n  args {\n    state {\n      recent_messages []\n    }\n    query {\n      message \"What is the weather?\"\n      timestamp 1715222400\n    }\n  }\n  @@assert( {{ this[0].type == \"message_to_user\" }})\n  @@assert( {{ this[0].message|regex_match(\"location\") }})\n  @@assert( {{ this[1].type == \"resume\" }})\n}\n\ntest WeatherToolAfterLocation() {\n  functions [ChooseTools]\n  args {\n    state {\n      recent_messages [\n        {\n          role User\n          content \"What's the weather?\"\n          timestamp 1715222400\n        },\n        {\n          role Assistant\n          content \"Where are you located?\"\n          timestamp 1715222401\n        },\n        {\n          role User\n          content \"San Francisco\"\n          timestamp 1715222402\n        }\n      ]\n    }\n    query {\n        message \"What's the weather?\"\n        timestamp 1715222400\n    }\n  }\n  @@assert( {{ this[0].type == \"get_weather_report\" }})\n  @@assert( {{ this[1].type == \"resume\" }})\n}\n\ntest WeatherToolAfterWeather() {\n  functions [ChooseTools]\n  args {\n    state {\n      weather_report {\n        temperature 20\n        weather_status \"Clear sky\"\n        timestamp 1715222400\n      }\n      recent_messages [\n        {\n          role User\n          content \"What's the weather?\"\n          timestamp 1715222400\n        },\n        {\n          role Assistant\n          content \"Where are you located?\"\n          timestamp 1715222401\n        },\n        {\n          role User\n          content \"San Francisco\"\n          timestamp 1715222402\n        }\n      ]\n\n    }\n    query {\n        message \"What's the weather?\"\n        timestamp 1715222400\n    }\n  }\n  @@assert( {{ this[0].type == \"message_to_user\" }})\n}",
  "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> VertexAI {\n  provider vertex-ai\n  options {\n    model \"gemini-2.5-pro-preview-03-25\"\n    location \"us-central1\"\n    project_id \"complete-button-459317-b4\"\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT4oMini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
  "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.89.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode async\n}\n\ngenerator ui {\n    output_type \"typescript\"\n    output_dir \"../../frontend/src\"\n    version \"0.89.0\"\n    default_client_mode sync\n}",
  "summarize.baml": "function SummarizeMessages(messages: Message[]) -> string {\n  client CustomGPT4o\n  prompt #\"\n    You are a helpful chatbot having a conversation with the user.\n    That conversation can get long, and you need to summarize it to\n    prevent your context from getting too long. Keep only the information\n    you expect to need for followup questions.\n\n    If the user says they will never mention something again,\n    that thing does not need to be in the summary. You don't\n    need to summarize the fact that they will never mention\n    in again. Strike it from the record.\n\n    Content:\n    {{ messages }}\n\n  \"#\n}\n\ntest Summarize() {\n    functions [SummarizeMessages]\n    args {\n      messages [\n        {\n          role User\n          content \"What is the weather in San Francisco?\"\n          timestamp 1715222400\n        },\n        {\n          role Assistant\n          content \"The weather in San Francisco is sunny and warm.\"\n          timestamp 1715222401\n        }\n      ]\n    }\n    @@assert( {{ this|regex_match(\"San Francisco\")}})\n}\n\ntest Disregard() {\n  functions [SummarizeMessages]\n  args {\n    messages [\n      {\n        role User\n        content \"I promise I will talk about juggling later, and I'm mentioning Baseball now, but I will never ask about it again.\"\n        timestamp 1715222400\n      },\n      {\n        role Assistant\n        content \"Ok\"\n        timestamp 1715222401\n      }\n    ]\n  }\n  @@assert( {{ this|regex_match(\"baseball\") == false }})\n}",
}
export const getBamlFiles = () => {
    return fileMap;
}